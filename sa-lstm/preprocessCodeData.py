from __future__ import print_function
from six.moves import xrange
import six.moves.cPickle as pickle

import gzip
import os
import nltk, re

import numpy
import theano


def prepare_data(seqs, labels, maxlen=None):
    lengths = [len(s) for s in seqs]

    if maxlen is not None:
        new_seqs = []
        new_labels = []
        new_lengths = []
        for l, s, y in zip(lengths, seqs, labels):
            if l < maxlen:
                new_seqs.append(s)
                new_labels.append(y)
                new_lengths.append(l)
        lengths = new_lengths
        labels = new_labels
        seqs = new_seqs

        if len(lengths) < 1:
            return None, None, None

    n_samples = len(seqs)
    maxlen = numpy.max(lengths)

    x = numpy.zeros((maxlen, n_samples)).astype('int64')
    x_mask = numpy.zeros((maxlen, n_samples)).astype(theano.config.floatX)
    for idx, s in enumerate(seqs):
        x[:lengths[idx], idx] = s
        x_mask[:lengths[idx], idx] = 1.

    return x, x_mask, labels


def load_data(path='imdb.pkl', n_words=1000000, valid_portion=0.1, maxlen=None, sort_by_len=True, dataFile=None):
    file_content = open(dataFile, 'r')
    dict={}
    if True:
        dictLines = open("data/java/dict.java").readlines()
        for line in dictLines:
            keyValue=line.split(',')
            dict[keyValue[0]]=keyValue[1]
    else:
        counter=0
        tokens = nltk.word_tokenize(file_content)
        for token in tokens:
            if token not in dict:
                dict[token] = counter
                counter+=1

    samples = []
    labels = []
    for line in file_content:
        smpl = []
        prog=None
        if 'badpositive' in line:
            labels.append(0)
            prog = re.sub(' badpositive','', line)
        else:
            labels.append(1)
            prog = re.sub(' goodpositive','', line)
        tokens = nltk.word_tokenize(prog)
        for token in tokens:
            if token in dict:
                smpl.append(dict[token])
            else:
                smpl.append(99999)
        samples.append(smpl)
    
    train_set = (samples, labels)
    test_set = list(train_set)
    
    # split training set into validation set
    train_set_x, train_set_y = train_set
    n_samples = len(train_set_x)
    sidx = numpy.random.permutation(n_samples)
    n_train = int(numpy.round(n_samples * (1. - valid_portion)))
    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]
    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]
    train_set_x = [train_set_x[s] for s in sidx[:n_train]]
    train_set_y = [train_set_y[s] for s in sidx[:n_train]]

    train_set = (train_set_x, train_set_y)
    valid_set = (valid_set_x, valid_set_y)

    def remove_unk(x):
        return [[1 if w >= n_words else w for w in sen] for sen in x]

    test_set_x, test_set_y = test_set
    valid_set_x, valid_set_y = valid_set
    train_set_x, train_set_y = train_set

    train_set_x = remove_unk(train_set_x)
    valid_set_x = remove_unk(valid_set_x)
    test_set_x = remove_unk(test_set_x)

    def len_argsort(seq):
        return sorted(range(len(seq)), key=lambda x: len(seq[x]))

    if sort_by_len:
        sorted_index = len_argsort(test_set_x)
        test_set_x = [test_set_x[i] for i in sorted_index]
        test_set_y = [test_set_y[i] for i in sorted_index]

        sorted_index = len_argsort(valid_set_x)
        valid_set_x = [valid_set_x[i] for i in sorted_index]
        valid_set_y = [valid_set_y[i] for i in sorted_index]

        sorted_index = len_argsort(train_set_x)
        train_set_x = [train_set_x[i] for i in sorted_index]
        train_set_y = [train_set_y[i] for i in sorted_index]

    train = (train_set_x, train_set_y)
    valid = (valid_set_x, valid_set_y)
    test = (test_set_x, test_set_y)

    return train, valid, test
