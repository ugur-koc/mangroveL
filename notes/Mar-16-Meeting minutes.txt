Ugur
	
	rerun scan-build on entire Juliet (improving the experiment)
		- got 6095 warnings (to be reduced)
	Run creduce + scan-build for each (warning, test case)
		- got 6095 snippets to be labeled
	Talked to Prof. Michael Hicks about two issues
		- can we preverse falseness of original warning through reduction?
		- can we find a way to automatically label these reduced snippets
			+ it turns out falseness depends on the falseness of original warning in original test case
				* the way we reduce Juliet test cases is very preservative
			
Zack
	high level description of false positives
	examples of false positives
	people from industry
	severity of false positives
	empirical 
	data from beyond ourselves
	

Abi
	clang tool
		 
Workshop paper ideas;
	Step one; trying out several ways of program reduction and creating a DB of snippets
		experimenting with creduce
			we tried several approaches to reduction
				- probabilistic approach
				- deterministic approach
					we need to show that this approach can actually work on real programs
		if we are only going mention about the success on reduction,
			then we can reduce number of snippets to ~300
			and we can add new SCA tools like coverity, codesonar, and cppcheck
				+ can we somehow publish results about these commercial tools?
			+ How about leveraging several SCA tool in reduction
				when for example, they all produce same false warning the reason can be important
				
	Learning patterns
		- where/why false positives convert to true positives?
			+ how can we leverage answer of this question in creating spam-filters
			
	Creating the spam-filters
		- Prof. Michael hicks suggested applying ML directly on P instead of reduced snippet
				+ Can it really work?
				+ is it easy to test?
				+ What can of features can/should we exract/look for?
		- Will these spam filters be warning/SCA specific?
		
				